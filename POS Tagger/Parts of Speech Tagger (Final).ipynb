{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting necessary feature to build the classifier.\n",
    "def Feature_Extraction(sentence, i):\n",
    "    features = {'token':sentence[i],'first_word': i == 0, 'capitalized':sentence[i][0].upper() == sentence[i][0],\n",
    "                'All_capitalized': sentence[i].upper() == sentence[i],'numeric': sentence[i].isdigit(),\n",
    "                'prev-word': '' if i == 0  else sentence[i - 1],'suffix(1)': sentence[i][-1],  \n",
    "                'suffix(2)': '' if len(sentence[i]) < 2  else sentence[i][-2:],\n",
    "                'suffix(3)': '' if len(sentence[i]) < 3  else sentence[i][-3:],'prefix(1)': sentence[i][0]\n",
    "               }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk                                                #importing nltk library and downloding brown dataset.\n",
    "\"\"\"nltk.download('brown')\"\"\"                                  \n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk import pos_tag\n",
    "tagged_sentence = brown.tagged_sents()\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sentence:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)                           #removing pos tags.\n",
    "    for i, (word, tag) in enumerate(tagged_sent):                         #segregation of word & tag from tagged sentence\n",
    "        featuresets.append( (Feature_Extraction(untagged_sent, i), tag) )  #calling function for feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating test set and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    }
   ],
   "source": [
    "#shuffling the dataset\n",
    "import random                             \n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets)) #size of dataset = 1161192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data set into test set and training set,test_set=20%,train_set=80%\n",
    "size = int(len(featuresets) * 0.2) #size=4339\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bniti\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training Linear Support Vector Classification with train data.\n",
    "import nltk.classify\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import classify \n",
    "classifier = nltk.classify.SklearnClassifier(LinearSVC())\n",
    "classifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify new data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press ENTER for given examples or Input your own sentence  .\n",
      " Output in the format of:\n",
      " {word:Pos_tag}\n",
      "\n",
      " {'And': 'CC', 'now': 'RB', 'for': 'IN', 'something': 'PN', 'completely': 'RB', 'different': 'JJ', ',': ',', 'Nitin': 'NP', 'lohith': 'NN', 'Harsha': 'NP', 'are': 'BER', 'in': 'IN', 'one': 'CD', 'team': 'NN', 'Input': 'JJ-TL', 'sentence': 'NN', 'or': 'CC', 'use': 'NN', 'default': 'NN', 'by': 'IN', 'pressing': 'VBG', 'enter': 'VB', 'Evaluate': 'NN-TL', 'the': 'AT', 'performance': 'NN', 'of': 'IN', 'model': 'NN', 'classifying': 'VBG', 'new': 'JJ', 'dataset': 'NN', 'Feature': 'NN-TL', 'Extraction': 'NN-TL', 'Function': 'NN-TL'} \n",
      "\n",
      "Output from nltk pos_tagger to compare our output:\n",
      "\n",
      " [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ'), (',', ','), ('Nitin', 'NNP'), ('lohith', 'NN'), ('Harsha', 'NNP'), ('are', 'VBP'), ('in', 'IN'), ('one', 'CD'), ('team', 'NN'), (',', ','), ('Input', 'NNP'), ('sentence', 'NN'), ('or', 'CC'), ('use', 'NN'), ('default', 'NN'), ('sentence', 'NN'), ('by', 'IN'), ('pressing', 'VBG'), ('enter', 'NN'), (',', ','), ('Evaluate', 'NNP'), ('the', 'DT'), ('performance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('model', 'NN'), ('in', 'IN'), ('classifying', 'VBG'), ('the', 'DT'), ('new', 'JJ'), ('dataset', 'NN'), (',', ','), ('Feature', 'NNP'), ('Extraction', 'NNP'), ('Function', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence=input(\"Press ENTER for given examples or Input your own sentence  .\")\n",
    "if len(sentence) < 1 : sentence = 'And now for something completely different , Nitin lohith Harsha are in one team , Input sentence or use default sentence by pressing enter , Evaluate the performance of the model in classifying the new dataset , Feature Extraction Function'\n",
    "tokenized_sentences = nltk.word_tokenize(sentence)\n",
    "words =[]\n",
    "tag = []\n",
    "for i,word in enumerate(tokenized_sentences):\n",
    "    words.append(word)\n",
    "    tag.append(classifier.classify(Feature_Extraction(tokenized_sentences,i )))\n",
    "output=dict(zip(words,tag))\n",
    "print(\" Output in the format of:\\n\",\"{word:Pos_tag}\\n\\n\",output,'\\n')\n",
    "\n",
    "print('Output from nltk pos_tagger to compare our output:\\n\\n',pos_tag(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate the performance of the model in classifying the new dataset(Test set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 94.23953013718686 (%)\n"
     ]
    }
   ],
   "source": [
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print (\"Accuracy of the model:\",accuracy*100,\"(%)\")   #Accuracy of the model: 94.23953013718686 (%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
